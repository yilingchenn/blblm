---
title: "blblm"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{blblm}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Package Description
This package has functions for running little bag of bootstraps on linear regression models and logistic regression model on datasets.

Bag of little bootstrap (BLB) is a procedure that incorporates bootstrap and resampling to prodce a computationally efficient, yet robust way of estimation.

For the linear regression model (blblm), users can determine the regression coefficients (beta heads), the estimate of variance of errors (sigma), confidence interval and prediction interval on new data.
    
For logistic regression model(blblr), users can determine the regression coefficients (beta heads), as well as confidence interval on particular coefficient or prediction interval on coefficients.
    
For both function inside the package, users can also decide if they want to use parallel computing by changing the option in function.

# Linear Regression (blblm)
Load the package and test it out:
```{r setup}
library(blblm)
```

## Design

The primary improvement added methods that support parallelization upon users' decision.

### Parallelization
```{r, eval = FALSE}
blblm <- function(formula, data, m = 10, B = 5000, parallel = FALSE, threads = 4) {
  # the function split_data put them into 
  data_list <- split_data(data, m)
  if (parallel){
    suppressWarnings(future::plan(multiprocess, workers = threads))
    estimates <- future_map(
      data_list,
      ~ lm_each_subsample(formula = formula, data = ., n = nrow(data), B = B))
  }else{
    estimates <- map(
      data_list,
      ~ lm_each_subsample(formula = formula, data = ., n = nrow(data), B = B))
  }
  res <- list(estimates = estimates, formula = formula)
  class(res) <- "blblm"
  invisible(res)
}
```

```future::plan(multiprocess, workers = threads)``` enables parallelization and in this case, the default threads/planner is at 4. if the users select parallel = TRUE. plan(multiprocess, workers = threads) is accompanied by future_map from library(furrr) for the computing

### Benchmarking

Generating data for the following examples:
```{r}
set.seed(100)
n = 10000
y = rnorm(n, mean = 10, sd = 2)
x = 0.5 * y + rnorm(n, mean = 0, sd = 0.25)
z = 0.3 * y + rnorm(n, mean = 0, sd = 0.25)
data = as.data.frame(cbind(x,y,z))
```

#### Parallelization

see improvement in speed (with moderate size dataset)
```{r, echo = FALSE}
library(furrr)
benchmark0 = bench::mark(
  # without parallelization
  blblm(y~x*z, data, m = 10, B = 5000),
  # with parallelization
  blblm(y~x*z, data, m = 10, B = 5000, parallel = TRUE, threads = 4),
  check = FALSE)
benchmark0
```

We can observe that by performing parallelization, the memory allocated is less and the number of iteration is larger using parallelization. The difference can be observe with larger amount of bootstraps and dataset.

#### 95 % Confidence Interval for Variable Coefficients

```{r, echo = FALSE}
fit_lm = blblm(y~x*z, data, m = 10, B = 1000)
fit_lm_par = blblm(y~x*z, data, m = 10, B = 1000, parallel = TRUE, threads = 4)
benchmark1 = bench::mark(
  # without parallelization for computing the model
  confint(fit_lm, level = 0.95),
  # with parallelization for computing the model
  confint(fit_lm_par, level = 0.95),
  confint(fit_lm, level = 0.95, parallel = TRUE, threads = 4),
  confint(fit_lm_par, level = 0.95, parallel = TRUE, threads = 4),
  check = FALSE
)
benchmark1
```
By computing parallely with the model, we see a lot less memory used compared to without using 

#### 95% Prediction Interval
```{r}
benchmark3 = bench::mark(
  predict(fit_lr, data.frame(x = c(0.2, 0.5), z = c(0.5, 0.7))),
  predict(fit_lr_par, data.frame(x = c(0.2, 0.5), z = c(0.5, 0.7))),
  predict(fit_lr, data.frame(x = c(0.2, 0.5), z = c(0.5, 0.7)), parallel = TRUE, threads = 4),
  predict(fit_lr_par, data.frame(x = c(0.2, 0.5), z = c(0.5, 0.7)), parallel = TRUE, threads = 4),
  check = FALSE
)

benchmark6
```


# Logistic Regression (blblr)

Similarly, we have BLB for logistic regression. Users are also allowed to decide if they want to do parallel computing for the estimation. 

```{r, eval = FALSE}
blblr <- function(formula, data, m = 10, B = 5000, parallel = FALSE, threads = 4){
  if (parallel){
    suppressWarnings(plan(multiprocess, workers = threads))
    options(future.rng.onMisuse = "ignore")
    data_list <- split_data(data, m)
    estimates <- future_map(data_list,~lr_each_subsample(formula = formula, data = ., n = nrow(data), B = B))
  }else{
    data_list <- split_data(data, m)
    estimates <- map(data_list, ~lr_each_subsample(formula = formula, data = ., n = nrow(data), B = B))
  }
  res <- list(estimates = estimates, formula = formula)
  class(res) <- "blblr"
  invisible(res)
}
```

### Benchmarking 

#### Data
First, we generate the sample data needed for running benchmark:
(note that the y needs to be in binary form in order to run logistic regression)
```{r}
set.seed(100)
n = 10000
y = rbinom(n, size=1, prob=0.05)
x = 0.5 * y + rnorm(n, mean = 0, sd = 0.25)
z = 0.3 * y + rnorm(n, mean = 0, sd = 0.25)
mydata = as.data.frame(cbind(x,y,z))
```

#### Parallelization

```{r, echo = FALSE}
benchmark4 = bench::mark(
  # without parallelization
  blblr(y~x*z, mydata, m = 10, B = 1000),
  # with parallelization
  blblr(y~x*z, mydata, m = 10, B = 1000, parallel = TRUE, threads = 4),
  check = FALSE)
benchmark4
```

#### 95 % Confidence Interval for Variable Coefficients

```{r, echo = FALSE}
fit_lr = blblr(y~x*z, mydata, m = 10, B = 1000)
fit_lr_par = blblr(y~x*z, mydata, m = 10, B = 1000, parallel = TRUE, threads = 4)
benchmark5 = bench::mark(
  confint(fit_lr, level = 0.95),
  confint(fit_lr, level = 0.95, parallel = TRUE, threads = 4),
  confint(fit_lr_par, level = 0.95),
  confint(fit_lr_par, level = 0.95, parallel = TRUE, threads = 4),
  check = FALSE
)

benchmark5
```

By computing parallely with the model, we see a lot less memory used compared to without using 

#### 95% Prediction Interval

```{r, eval = FALSE, echo = FALSE}
benchmark6 = bench::mark(
  predict(fit_lr, data.frame(x = c(0.2, 0.5), z = c(0.5, 0.7))),
  predict(fit_lr_par, data.frame(x = c(0.2, 0.5), z = c(0.5, 0.7))),
  predict(fit_lr, data.frame(x = c(0.2, 0.5), z = c(0.5, 0.7)), parallel = TRUE, threads = 4),
  predict(fit_lr_par, data.frame(x = c(0.2, 0.5), z = c(0.5, 0.7)), parallel = TRUE, threads = 4),
  check = FALSE
)

benchmark6
```